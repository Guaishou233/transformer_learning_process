{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 构建transformer流程学习记录",
   "id": "a3edffcfbc53dd41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##embedding 前的输入转化，为什么文字能输入到模型",
   "id": "3a9626f3f8469a33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from os import close\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.ao.nn.quantized import LayerNorm\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.functional import dropout\n",
    "\n",
    "# 构建词汇表\n",
    "vocab = {\"<pad>\": 0, \"hello\": 1, \"world\": 2, \"this\": 3, \"is\": 4, \"a\": 5, \"test\": 6}\n",
    "vocab_size = len(vocab)  # 词汇表大小\n",
    "embedding_dim = 5  # 嵌入向量的维度\n",
    "\n",
    "# 创建嵌入层\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# 将文本转换为数值索引\n",
    "text = \"hello world this is\"\n",
    "indices = [vocab[word] for word in text.split()]  # 数值化后的输入\n",
    "x = torch.tensor(indices)  # 转换为张量\n",
    "\n",
    "# 使用嵌入层\n",
    "embedded_vectors = embedding(x)\n",
    "\n",
    "print(\"输入文本:\", text)\n",
    "print(\"数值化后的输入:\", x)\n",
    "print(\"嵌入矩阵的形状:\", embedding.weight.shape)\n",
    "print(\"输出的嵌入向量:\", embedded_vectors)"
   ],
   "id": "3ab6be2e24ada944"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 构建embedding 层\n",
    "本质上embedding层是一个词汇表，将文字输入embedding层的作用就是将转化为索引后的汉字变为一个自定义维度的向量（常用的是512）"
   ],
   "id": "6bb79165a53b33f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import math\n",
    "#embedding\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        #这里是在创建一个vocab_size行embedding_dim列的嵌入层张量，也可以理解为一个索引为vocab_size,value为embedding_dim的字典。\n",
    "        self.embedding_dim = embedding_dim\n",
    "        #这里是为了在后面缩放输入，是的输入的数据在训练过程中稳定。\n",
    "        #但是我差一个例子来说明，如果没有这个缩放过程会有是吗影响\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.ebedding_dim)\n",
    "        #这里x是经过tokenizer转化后的索引（数字）,embedding(x)的作用就是将x的里面的索引数字和embedding层里的索引进行匹配然后返回一个embedding_dim维度的向量。简单理解就是embedding(x)的作用就是在查表。"
   ],
   "id": "4f9f05ea519139cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 位置编码\n",
    "为每个向量加上位置信息"
   ],
   "id": "b9a639c5a60e283f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#创建位置编码\n",
    "class positional_encoding(nn.Module):\n",
    "    def __init__(self,d_dim,dropout, max_len = 5000):\n",
    "        super(positional_encoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position_encoding_matrix = np.zeros((max_len,d_dim))\n",
    "        # 创建每个输入大小的位置编码\n",
    "        position = torch.arange(0,d_dim).unsqueeze(dim = 0)\n",
    "        # 创建一个(1，d_dim)的张量方便后续通过广播计算\n",
    "        div_term = math.exp(- torch.arange(0,d_dim,2) * math.log(10000.0) / d_dim)\n",
    "        # 公式的基础写法\n",
    "        position_encoding_matrix[:, 0::2] = torch.sin(position * div_term)\n",
    "        position_encoding_matrix[:, 1::2] = torch.cos(position * div_term)\n",
    "        position_encoding_matrix = position_encoding_matrix.unsqueeze(dim = 0)\n",
    "        # 这里unsqueeze的作用是为其增加一个batch_size的维度是的这个位置编码可以通过广播机制加载到每个批次的输入上\n",
    "        self.register_buffer('position_encoding_matrix', position_encoding_matrix)\n",
    "        # 这里使用register_buffer是为了保障其不会被优化器改变\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.position_encoding_matrix[:,x.size(1)]\n",
    "        # 这里的x.size是为x匹配对应长度的位置编码\n",
    "        return self.dropout(x)"
   ],
   "id": "3d232fce2f1ca3c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 构建解码器",
   "id": "94ad38a28308588a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size, d_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_dim, vocab_size)\n",
    "        # 主要作用是把上一层的输出的每个序列对应的高纬度特征变为与词表的对应数值分布。然后在下面的softmax操作中变为对应概率分布，方便后续输出结果。\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ],
   "id": "a2bf82e88ee7eeba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import copy\n",
    "\n",
    "# 构建层复制器\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ],
   "id": "d0a49332edb1a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 构建编码器\n",
    "# 既然解码器里有多个层，每个层又有两个子层，分别是自注意力层和前馈全连接层，每个子层后面跟着一个残差网络和归一化。为什么在encoder里没有体现？而只是简单的进行了最后一次的layernorm?\n",
    "# 编码器里没有直接写编码层而是另外写了一个编码层类\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = nn.LayerNorm(layer.size)\n",
    "        # LayerNorm的作用和执行？\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mask)\n",
    "        return self.norm(x)"
   ],
   "id": "4d631821a51cef90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 构建编码层\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.size = size\n",
    "        # 设置这个size是干嘛的\n",
    "        self.sublayer = clones(SublayerConnection((size,dropout), 2))\n",
    "        # 这个sublayerconnection又自己实现，包含了 残差网络和归一化层在里面\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x:self.self_attn(x,x,x,mask))\n",
    "        z = self.sublayer[1](x, self.feed_forward)\n",
    "        # 为什么这里换成了Z?\n",
    "        return z\n"
   ],
   "id": "8dea3abe93fc8b04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# 还差一个自注意力机制的实现代码",
   "id": "4092abb79342bd71"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
