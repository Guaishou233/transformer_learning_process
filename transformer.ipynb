{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 构建transformer流程学习记录",
   "id": "a3edffcfbc53dd41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##embedding 前的输入转化，为什么文字能输入到模型",
   "id": "3a9626f3f8469a33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from os import close\n",
    "from sqlite3.dbapi2 import sqlite_version_info\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sympy import sequence\n",
    "from torch.ao.nn.quantized import LayerNorm\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.functional import dropout\n",
    "\n",
    "# 构建词汇表\n",
    "vocab = {\"<pad>\": 0, \"hello\": 1, \"world\": 2, \"this\": 3, \"is\": 4, \"a\": 5, \"test\": 6}\n",
    "vocab_size = len(vocab)  # 词汇表大小\n",
    "embedding_dim = 5  # 嵌入向量的维度\n",
    "\n",
    "# 创建嵌入层\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# 将文本转换为数值索引\n",
    "text = \"hello world this is\"\n",
    "indices = [vocab[word] for word in text.split()]  # 数值化后的输入\n",
    "x = torch.tensor(indices)  # 转换为张量\n",
    "\n",
    "# 使用嵌入层\n",
    "embedded_vectors = embedding(x)\n",
    "\n",
    "print(\"输入文本:\", text)\n",
    "print(\"数值化后的输入:\", x)\n",
    "print(\"嵌入矩阵的形状:\", embedding.weight.shape)\n",
    "print(\"输出的嵌入向量:\", embedded_vectors)"
   ],
   "id": "3ab6be2e24ada944"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 构建embedding 层\n",
    "本质上embedding层是一个词汇表，将文字输入embedding层的作用就是将转化为索引后的汉字变为一个自定义维度的向量（常用的是512）"
   ],
   "id": "6bb79165a53b33f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import math\n",
    "#embedding\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        #这里是在创建一个vocab_size行embedding_dim列的嵌入层张量，也可以理解为一个索引为vocab_size,value为embedding_dim的字典。\n",
    "        self.embedding_dim = embedding_dim\n",
    "        #这里是为了在后面缩放输入，是的输入的数据在训练过程中稳定。\n",
    "        #但是我差一个例子来说明，如果没有这个缩放过程会有是吗影响\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.ebedding_dim)\n",
    "        #这里x是经过tokenizer转化后的索引（数字）,embedding(x)的作用就是将x的里面的索引数字和embedding层里的索引进行匹配然后返回一个embedding_dim维度的向量。简单理解就是embedding(x)的作用就是在查表。"
   ],
   "id": "4f9f05ea519139cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 位置编码\n",
    "为每个向量加上位置信息"
   ],
   "id": "b9a639c5a60e283f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#创建位置编码\n",
    "class positional_encoding(nn.Module):\n",
    "    def __init__(self,d_dim,dropout, max_len = 5000):\n",
    "        super(positional_encoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position_encoding_matrix = np.zeros((max_len,d_dim))\n",
    "        # 创建每个输入大小的位置编码\n",
    "        position = torch.arange(0,d_dim).unsqueeze(dim = 0)\n",
    "        # 创建一个(1，d_dim)的张量方便后续通过广播计算\n",
    "        div_term = math.exp(- torch.arange(0,d_dim,2) * math.log(10000.0) / d_dim)\n",
    "        # 公式的基础写法\n",
    "        position_encoding_matrix[:, 0::2] = torch.sin(position * div_term)\n",
    "        position_encoding_matrix[:, 1::2] = torch.cos(position * div_term)\n",
    "        position_encoding_matrix = position_encoding_matrix.unsqueeze(dim = 0)\n",
    "        # 这里unsqueeze的作用是为其增加一个batch_size的维度是的这个位置编码可以通过广播机制加载到每个批次的输入上\n",
    "        self.register_buffer('position_encoding_matrix', position_encoding_matrix)\n",
    "        # 这里使用register_buffer是为了保障其不会被优化器改变\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.position_encoding_matrix[:,x.size(1)]\n",
    "        # 这里的x.size是为x匹配对应长度的位置编码\n",
    "        return self.dropout(x)"
   ],
   "id": "3d232fce2f1ca3c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 构建解码器",
   "id": "94ad38a28308588a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size, d_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_dim, vocab_size)\n",
    "        # 主要作用是把上一层的输出的每个序列对应的高纬度特征变为与词表的对应数值分布。然后在下面的softmax操作中变为对应概率分布，方便后续输出结果。\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ],
   "id": "a2bf82e88ee7eeba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import copy\n",
    "\n",
    "# 构建层复制器\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ],
   "id": "d0a49332edb1a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 构建编码器\n",
    "# 既然解码器里有多个层，每个层又有两个子层，分别是自注意力层和前馈全连接层，每个子层后面跟着一个残差网络和归一化。为什么在encoder里没有体现？而只是简单的进行了最后一次的layernorm?\n",
    "# 编码器里没有直接写编码层而是另外写了一个编码层类\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = nn.LayerNorm(layer.size)\n",
    "        # LayerNorm的作用和执行？\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mask)\n",
    "        return self.norm(x)"
   ],
   "id": "4d631821a51cef90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 构建编码层\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.size = size\n",
    "        # 设置这个size是干嘛的\n",
    "        self.sublayer = clones(SublayerConnection((size,dropout), 2))\n",
    "        # 这个sublayerconnection又自己实现，包含了 残差网络和归一化层在里面\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x:self.self_attn(x,x,x,mask))\n",
    "        z = self.sublayer[1](x, self.feed_forward)\n",
    "        # 为什么这里换成了Z?\n",
    "        return z\n"
   ],
   "id": "8dea3abe93fc8b04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#自注意力机制的实现代码\n",
    "def attention(query,key,value,mask = None, dropout = None):\n",
    "    # 这里qkv不再进行线性变化直接输入是因为在下面的多头注意力执行前已经进行了线性变换。所以注意这里不是原始的X而是经过线性变化的QKV\n",
    "    d_k = query.size(-1)\n",
    "    # 这里d_k 的值用Q或者K的都可以\n",
    "    scores = torch.matmul(query,key.transpose(-1,-2)) / math.sqrt(d_k)\n",
    "    # 将QK的任意一个的倒数第一二维度进行转置，是的QK计算后的到一个形状为(bs,seq_len,seq_len)的张量((sel_len,sel_len)实际就是每个序列间的注意力权重)，这个张量就是经过softmax前的注意力机制的权重\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    # 这里加入mask是为了直接将不相关的元素排除掉，通过设置为-1e9使其在后面的softmax操作中的到的值很小\n",
    "\n",
    "    p_attention = torch.softmax(scores, dim = -1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        p_attention = dropout(p_attention)\n",
    "\n",
    "    return torch.matmul(p_attention, value), p_attention\n",
    "# 暂时还不明白为什么除了输出经过注意力权重和value计算后的结果后还要输出新的注意力权重\n",
    "# 这里的注意力权重和输出将用于后面的多头注意力进行后面的计算"
   ],
   "id": "4092abb79342bd71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 为什么多头就可以提升效果？单个的d_k为64 多个的为八个头 单个的d_k = 8\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout = 0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        # 这里是确保每个序列的高纬度特征能被平均的分配个每个注意力头\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        # 每个QKV输入需要进行线性变化，同时最后从新整合好的输出也需要线性变换所以这里是四个\n",
    "        self.attn = None\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        # 注意这里的QKV都是一样的X在后面经过线性层的变换后才是真正的QKV\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query,key,value = [linear(x).view(batch_size,-1,self.n_heads,self.d_k).transpose(1,2) for linear,x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        output,self.attn = attention(query,key,value,mask = mask, dropout = self.dropout)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k)\n",
    "        # 这里再次使用转置transpose(1, 2)是因为之前变换为多头的时候改变了内存的元数据，导致每一个元素的元数据实际上物理地址是不连续的，这里再次使用相同的转置使得元素在内存上是连续的，同时并将其变回原来的输入的样子。\n",
    "\n",
    "        return self.linears[-1](output)\n",
    "        # 这里使用最后一个线性层进行线性变换\n",
    "\n",
    "\n"
   ],
   "id": "c9198ef5cc1006d6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
