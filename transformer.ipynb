{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 构建transformer流程学习记录",
   "id": "a3edffcfbc53dd41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##embedding 前的输入转化，为什么文字能输入到模型",
   "id": "3a9626f3f8469a33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# 构建词汇表\n",
    "vocab = {\"<pad>\": 0, \"hello\": 1, \"world\": 2, \"this\": 3, \"is\": 4, \"a\": 5, \"test\": 6}\n",
    "vocab_size = len(vocab)  # 词汇表大小\n",
    "embedding_dim = 5  # 嵌入向量的维度\n",
    "\n",
    "# 创建嵌入层\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# 将文本转换为数值索引\n",
    "text = \"hello world this is\"\n",
    "indices = [vocab[word] for word in text.split()]  # 数值化后的输入\n",
    "x = torch.tensor(indices)  # 转换为张量\n",
    "\n",
    "# 使用嵌入层\n",
    "embedded_vectors = embedding(x)\n",
    "\n",
    "print(\"输入文本:\", text)\n",
    "print(\"数值化后的输入:\", x)\n",
    "print(\"嵌入矩阵的形状:\", embedding.weight.shape)\n",
    "print(\"输出的嵌入向量:\", embedded_vectors)"
   ],
   "id": "3ab6be2e24ada944"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 构建embedding 层\n",
    "本质上embedding层是一个词汇表，将文字输入embedding层的作用就是将转化为索引后的汉字变为一个自定义维度的向量（常用的是512）"
   ],
   "id": "6bb79165a53b33f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import math\n",
    "#embedding\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        #这里是在创建一个vocab_size行embedding_dim列的嵌入层张量，也可以理解为一个索引为vocab_size,value为embedding_dim的字典。\n",
    "        self.embedding_dim = embedding_dim\n",
    "        #这里是为了在后面缩放输入，是的输入的数据在训练过程中稳定。\n",
    "        #但是我差一个例子来说明，如果没有这个缩放过程会有是吗影响\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.ebedding_dim)\n",
    "        #这里x是经过tokenizer转化后的索引（数字）,embedding(x)的作用就是将x的里面的索引数字和embedding层里的索引进行匹配然后返回一个embedding_dim维度的向量。简单理解就是embedding(x)的作用就是在查表。"
   ],
   "id": "4f9f05ea519139cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 位置编码\n",
    "为每个向量加上位置信息"
   ],
   "id": "b9a639c5a60e283f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#创建位置编码\n",
    "class positional_encoding(nn.Module):\n",
    "    def __init__(self,d_dim,dropout, max_len = 5000):\n",
    "        super(positional_encoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position_encoding_matrix = np.zeros((max_len,d_dim))\n",
    "        # 创建每个输入大小的位置编码\n",
    "        position = torch.arange(0,d_dim).unsqueeze(dim = 0)\n",
    "        # 创建一个(1，d_dim)的张量方便后续通过广播计算\n",
    "        div_term = math.exp(- torch.arange(0,d_dim,2) * math.log(10000.0) / d_dim)\n",
    "        # 公式的基础写法\n",
    "        position_encoding_matrix[:, 0::2] = torch.sin(position * div_term)\n",
    "        position_encoding_matrix[:, 1::2] = torch.cos(position * div_term)\n",
    "        position_encoding_matrix = position_encoding_matrix.unsqueeze(dim = 0)\n",
    "        # 这里unsqueeze的作用是为其增加一个batch_size的维度是的这个位置编码可以通过广播机制加载到每个批次的输入上\n",
    "        self.register_buffer('position_encoding_matrix', position_encoding_matrix)\n",
    "        # 这里使用register_buffer是为了保障其不会被优化器改变\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.position_encoding_matrix[:,x.size(1)]\n",
    "        # 这里的x.size是为x匹配对应长度的位置编码\n",
    "        return self.dropout(x)"
   ],
   "id": "3d232fce2f1ca3c9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
